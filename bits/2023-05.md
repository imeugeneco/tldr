# 2023-05

[052723] [API V2 전환과 DB 무중단 마이그레이션 후기](https://medium.com/29cm/api-v2-전환과-db-무중단-마이그레이션-후기-8b39eb0db566) 

- 좋아요 API의 V2 전환 & PostgreSQL 데이터를 MySQL로 무중단 마이그레이션 진행
- 데이터 행이 많아 시간 단위로 마이그레이션을 진행해야 했기에 서비스를 중단, 기존 데이터를 dump, 신규 시스템에 load하는 방법이 아닌 서비스 중단 없이 데이터를 신규 시스템에 스트리밍하여 동기화
- V1 인터페이스를 그대로 마이크로서비스에 포팅하는 대신, V1 API에서 좋아요 설정/해제 시 V2 API에 동기화가 가능하도록 이벤트를 발송하는 구조

나도 곧 100만 행을 훌쩍 넘는 찰리 로그 테이블 마이그레이션을 해야하는데 다른 곳들은 어떻게 했는지 궁금해서 DB 마이그레이션 후기 찾아보면서 본 글. 저번 마이그레이션때 파일이 꼬이면서 개발과 프로덕션 환경이 서로 맞지 않게된 것도 있고, 중단 걸고 할 정도는 아닌 것 같지만 경험도 없는데다가 호출이 가장 자주 일어나는 테이블 중 하나라 좀 막막하다.

---

[052823] [CORS는 왜 이렇게 우리를 힘들게 하는걸까?](https://evan-moon.github.io/2020/05/21/about-cors/)

* 다른 출처로의 리소스 요청을 제한하는 것과 관련된 두 가지 정책: `CORS (Cross-Origin Resource Sharing)` 와 `SOP(Same-Origin Policy)`
  * `SOP`: 같은 출처에서만 리소스를 공유할 수 있음
  * `CORS`: CORS 정책을 지킨 리소스 요청은 출처가 다르더라도 허용

* `Scheme`, `Host`, `Port`, 이 3가지가 동일하면 같은 출처라고 판단 ([RFC 6454](https://datatracker.ietf.org/doc/html/rfc6454#section-5))
  * 출처를 비교하는 로직은 서버가 아니라 브라우저에 구현된 스펙

* **CORS 동작 플로우**
  * 웹 클라이언트 어플리케이션이 다른 출처의 리소스를 요청할 때는 HTTP 프로토콜을 사용하여 요청을 보내게 되는데, 이때 브라우저는 요청 헤더에 `Origin`이라는 필드에 요청을 보내는 출처를 함께 담아보냄
  * 이후 서버가 이 요청에 대한 응답을 할 때 응답 헤더의 `Access-Control-Allow-Origin`이라는 값에 “이 리소스를 접근하는 것이 허용된 출처”를 내려주고, 이후 응답을 받은 브라우저는 자신이 보냈던 요청의 `Origin`과 서버가 보내준 응답의 `Access-Control-Allow-Origin`을 비교해본 후 이 응답이 유효한 응답인지 아닌지를 결정


개발 편의성을 위해 평소 토이 프로젝트 할 때 `access-control-allow-origin` : `*` 을 넣어줬었는데 김무령한테 혼나고 공부중 ㅜㅜ 

---

[052923] **찰리 개발하면서 배운 것들**

몇 달 전 찰리 서버랑 혼자 4~5시간 씨름하고서 썼던 트윗을 오랜만에 봤다. 그래서 써보는 (아무도 알려주지 않았지만) 찰리 개발/운영/보수하면서 배운 것들 몇 가지 

**CI/CD**

* 주기적으로, 자주 배포하는 게 앱 퀄리티 차이를 만들어낸다는 얘기는 많이 들어봤지만, 깃허브에 푸시할 때마다 자동으로 배포를 해 주고 호스팅까지 해 주는 서비스들이 있다는 것은 몰랐다. CD가 갖춰지니까 배포 과정이 굉장히 편하다. 

**Logging**

* 갑작스럽게 몇천 명의 유저 유입이 들어오는데 서버는 불안정하고, 심지어 내가 볼 수 있는 에러 로그는 없는 상황. 트위터 디엠으로 몇백 개의 디엠이 쏟아지는데, 심장이 턱 걸린 느낌이었다. 서버 살려놓고 제일 먼저 한 일은 에러 로깅처리. 이제 에러 나면 로그 파일부터 확인!
* 
  근데 이것보다 800개가 넘는 "찰리 서버 죽었나요?" 디엠에 하나 하나 답장하는 게 더 힘들었다. 비슷한 내용으로 메세지 10개 정도 보내면 15~20분 동안은 디엠을 보낼 수 없다...

**Monitoring & Debugging**

* 이것도 서버 몇 번 터뜨리고 배운... EC2 인스턴스의 디스크 사용률이 70, 80, 90%를 넘어버리더니 픽 죽어버렸다. 중간중간에 서버를 들여다볼 시간과 기회는 충분했는데 알림을 너무 늦게 봐서 놓친 것.
* 그런데 SSH 연결과 콘솔을 통한 인스턴스 접속이 되지 않아 네트워크랑 보안 세팅도 다시 보고, 용량 문제일까 싶어 EBS 늘려보고 인스턴스 타입을 업그레이드해 보아도 해결되지 않았다. 인스턴스를 아무리 여러 번 리 부팅시켜봐도 인스턴스 상태 점검이 통과하지 않아 내부 메모리나 볼륨 조회 자체를 못 하는 상황. 
* 기본 용량이 다 차버리거나 과부하가 걸려 메모리 쓰는 부분까지 갑자기 다 차면 이런 참사가 일어나는구나 확실히 배웠고 나중에 이런 경우 인스턴스 환경에서 서버 재시작 후 재빠르게 SSH/FTP 접속을 해서 적당한 용량의 파일을 지우면 해결할 수 있다는 것도 배웠다 (사태 수습 후 보니 다량의 쿼리 누적이 문제). 너무 다행히도 위에서 로그의 중요성을 깨닫고 에러 처리할 때 로그를 남기도록 해둔 상태라 문제점 파악과 같은 상황 예방을 위한 스텝을 밟는데 수월했다.
